%Jennifer Pan, August 2011

\documentclass[10pt,letter]{article}
	% basic article document class
	% use percent signs to make comments to yourself -- they will not show up.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{enumitem}
	% packages that allow mathematical formatting

\usepackage{graphicx}
	% package that allows you to include graphics

\usepackage{setspace}
	% package that allows you to change spacing

\onehalfspacing
	% text become 1.5 spaced

\usepackage{fullpage}
	% package that specifies normal margins


\begin{document}
	% line of code telling latex that your document is beginning


\title{ECON550: Problem Set 8}

\author{Nicholas Wu}

\date{Fall 2020}
	% Note: when you omit this command, the current dateis automatically included

\maketitle
	% tells latex to follow your header (e.g., title, author) commands.
\section*{Midterm Problem 6}

\begin{enumerate}[label=(\alph*)]
\item Define the statistic $\hat{\theta}$ as
\[ \hat{\theta} = n^{-1}\sum_{i=1}^n \log\left(\frac{ X_i}{Y_i}\right) - \log\left(\frac{n^{-1}\sum_{i=1}^n X_i}{n^{-1}\sum_{i=1}^n Y_i}\right) \]
By the WLLN,
\[ n^{-1}\sum_{i=1}^n \log\left(\frac{ X_i}{Y_i}\right) \to_p E\log\left(\frac{ X_i}{Y_i}\right) \]
\[ n^{-1}\sum_{i=1}^n X_i \to_p EX_i \]
\[ n^{-1}\sum_{i=1}^n Y_i \to_p EY_i \]
We require $E\log\left(\frac{ X_i}{Y_i}\right) < \infty$, $EX_i < \infty$, $EY_i < \infty$. Now, by Slutsky's theorem and the rules of convergence in probability, we get
\[ \log\left(\frac{n^{-1}\sum_{i=1}^n X_i}{n^{-1}\sum_{i=1}^n Y_i}\right) \to_p \log \left( \frac{EX_i}{EY_i} \right)\]
Applying the rules of convergence in probability, we get
\[ \hat{\theta} = n^{-1}\sum_{i=1}^n \log\left(\frac{ X_i}{Y_i}\right) - \log\left(\frac{n^{-1}\sum_{i=1}^n X_i}{n^{-1}\sum_{i=1}^n Y_i}\right) \]
\[ \to_p E\log\left(\frac{ X_i}{Y_i}\right) + \log \left( \frac{EX_i}{EY_i} \right) \]
\item Define $g$ as
\[ g(A, B, C) = A - \log(B/C) \]
Define
\[ Z_i = \left(\begin{bmatrix} \log(X_i / Y_i) \\ X_i \\ Y_i \end{bmatrix} \right) \]
Let
\[ \bar{Z_i} = n^{-1} \sum_{i=1}^n Z_i \]
Then
\[ \hat{\theta} = g(\bar{Z_i}) \]
Note that
\[ \theta = g(EZ_i) =  E\log\left(\frac{ X_i}{Y_i}\right) + \log \left( \frac{EX_i}{EY_i} \right)\]
Define the variance matrix $\Sigma$ as the variance matrix $Var(Z_i)$.
By the CLT,
\[ \sqrt{n} (\bar{Z_i} - EZ_i ) \to_d N(0,\Sigma) \]
By the delta method,
\[ G(\theta)= \begin{bmatrix} 1 & -1/EX_i & 1/EX_y \end{bmatrix} \]
so
\[ \sqrt{n} (\hat{\theta} - \theta) = \sqrt{n}(g(\bar{Z_i}) - EZ_i) \to_d N(0, G(\theta)\Sigma G(\theta)') \]
\item We can estimate $G(\theta)$ with
\[ \hat{G} = \begin{bmatrix} 1 & -1/\bar{X}_i & 1/\bar{Y}_i \end{bmatrix} \]
By Slutsky's theorem, since $\bar{X}_i \to_p EX_i$ and $\bar{Y}_i \to_p EY_i$,
\[ \hat{G} = \begin{bmatrix} 1 & -1/\bar{X}_i & 1/\bar{Y}_i \end{bmatrix} \to_p  \begin{bmatrix} 1 & -1/EX_i & 1/EX_y \end{bmatrix} =G(\theta) \]
Now, we just need to estimate $\Sigma$. We can take
\[ \hat{\Sigma} = n^{-1} \sum_{i=1}^N Z_iZ_i' - \bar{Z_i}\bar{Z_i}' \]
Assuming $||EZ_iZ_i'||, |EZ_i| < \infty$, by the WLLN,
\[ n^{-1} \sum_{i=1}^N Z_iZ_i' \to EZ_iZ_i' \]
Therefore, by Slutsky's theorem
\[ \hat{\Sigma} \to_p  \Sigma \]
Hence, applying Slutsky's theorem again,
\[ \hat{G} \hat{\Sigma}  \hat{G}'\]
converges in probability to the desired variance.
\end{enumerate}
\section*{Midterm Problem 7}
\begin{enumerate}[label=(\alph*)]
\item We have
\[ \frac{\bar{Y_i}}{\bar{X_i}} = \frac{\sqrt{n}\bar{Y_i}}{\sqrt{n}\bar{X_i}}\]
Define
\[ Z_i = \begin{bmatrix} Y_i \\ X_i \end{bmatrix} \]
Note
\[ EZ_i = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \]
Let $\Sigma = Var(Z_i)$. We have by the CLT
\[ \sqrt{n} \bar{Z_i}  \to_d \begin{bmatrix} N_1 \\ N_2 \end{bmatrix} \sim N(0, \Sigma)  \]
Let $g(A, B) = A/B$. Note
\[ \frac{\bar{Y_i}}{\bar{X_i}} = g(\bar{Z_i}) \]
Then by the CMT,
\[ \frac{\bar{Y_i}}{\bar{X_i}} = \frac{\sqrt{n}\bar{Y_i}}{\sqrt{n}\bar{X_i}} \to_d g(N(0, \Sigma)) = \frac{N_1}{N_2} \]
\item We consider
\[ \frac{\bar{Y_i}}{\sqrt{n}\bar{X_i}} \]
We know $\bar{Y_i} \to_p \mu_Y$. Applying the rules for convergence in distribution, we get, via the CMT again,
\[ \frac{\bar{Y_i}}{\sqrt{n}\bar{X_i}} \to_d \frac{\mu_Y}{N_2}\]
where $N_2$ is from the previous part.

\end{enumerate}
\section*{Problem 1}
\[ \hat{\beta}_n = \left(\sum_{i=1}^n X_iX_i' \right)^{-1} \sum_{i=1}^n X_iY_i \]
\[ = \left(n^{-1}\sum_{i=1}^n X_iX_i' \right)^{-1} \left (n^{-1} \sum_{i=1}^n X_iY_i \right) \]
Note by WLLN
\[ \left(n^{-1}\sum_{i=1}^n X_iX_i' \right)^{-1} \to_p E[X_iX_i'] \]
\[ \left (n^{-1} \sum_{i=1}^n X_iY_i \right) \to_p E[X_i Y_i]\]
Applying Slutsky and the rules of convergence in probability, and using the fact that $\Sigma$ is positive definite,
\[ \hat{\beta}_n = \left(n^{-1}\sum_{i=1}^n X_iX_i' \right)^{-1} \left (n^{-1} \sum_{i=1}^n X_iY_i \right) \]
\[ \to_p (E[X_iX_i'])^{-1} E[X_i Y_i] \]
\[ = (E[X_iX_i'])^{-1} E[X_i X_i' \beta + X_i U_i] \]
\[ = \beta + (E[X_iX_i'])^{-1}  E[X_i U_i] \]
Note that the other term will generally not be zero if $E[U_i |X_i]$ is not zero.
\section*{Problem 2}
\begin{enumerate}[label=(\alph*)]
\item We have
\[ \sqrt{n} (\hat{beta_n} - \beta_0) = \sqrt{n}\left( (X'X) ^{-1}(X'Y) - \beta_0 \right) \]
\[ = \sqrt{n}\left( (X'X) ^{-1}(X'(X\beta_0 + U)) - \beta_0 \right) \]
\[ = \sqrt{n}\left( (X'X) ^{-1}(X'X\beta_0 + X'U)) - \beta_0 \right) \]
\[ = \sqrt{n}\left( (X'X) ^{-1}(X'U)\right) \]
\[ = \sqrt{n}\left( \left(\frac{1}{n}\sum_{i=1}^n X_iX_i' \right) ^{-1}\left(\frac{1}{n}\sum_{i=1}^n X_iU_i \right)\right) \]
Now
\[\left(\frac{1}{n}\sum_{i=1}^n X_iX_i' \right)  \to_p \Sigma_X \]
since $\Sigma_X$ is positive definite, by Slutsky's theorem,
\[ \left(\frac{1}{n}\sum_{i=1}^n X_iX_i' \right)^{-1}  \to_p \Sigma_X^{-1} \]
Additionally, since $E[X_iU_i] = 0$, by the CLT
\[\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n X_iU_i \right) \to_d N(0, Var(X_iU_i)) \]
So all together
\[ \sqrt{n} (\hat{beta_n} - \beta_0) = \sqrt{n}\left( \left(\frac{1}{n}\sum_{i=1}^n X_iX_i' \right) ^{-1}\left(\frac{1}{n}\sum_{i=1}^n X_iU_i \right)\right) \to_d N(0, \Sigma_X^{-1} Var(X_iU_i)(\Sigma_X^{-1})') \]\[= N(0, \Sigma_X^{-1} E(U_iX_iX_i'U_i)(\Sigma_X^{-1})')\]
\item  The variance is $\Sigma_X^{-1} E(U_iX_iX_i'U_i)(\Sigma_X^{-1})')$. To get a consistent estimator of this expression, we just need a consistent estimator for $\Sigma_X$, and a consistent estimator for $E(U_iX_iX_i'U_i)$, and we can then apply the rules of convergence in probability and Slutsky's theorem to get the overall variance estimator (since $\Sigma_X$ is positive definite).

By the WLLN, we already know
\[ n^{-1} \sum X_iX_i' \to_p \Sigma_X \]
Hence we just have to construct an estimator for $E(U_iX_iX_i'U_i)$. Consider estimating the $(a,b)$ element as
\[ \hat{\Sigma}_{ab} = n^{-1} \sum_{i=1}^n (Y_i - X_i'\hat{\beta})X_{ai}X_{bi}(Y_i - X_i'\hat{\beta}) \]
 \[ = n^{-1} \sum_{i=1}^n (X_i'(\beta_0 - \hat{\beta}) + U_i)X_{ai}X_{bi}( X_i'(\beta_0 - \hat{\beta}) + U_i)  \]
  \[ = n^{-1} \sum_{i=1}^n X_i'(\beta_0 - \hat{\beta})X_{ai}X_{bi} X_i'(\beta_0 - \hat{\beta}) + U_iX_{ai}X_{bi}X_i'(\beta_0 - \hat{\beta}) + X_i'(\beta_0 - \hat{\beta})X_{ai}X_{bi}U_i + U_iX_{ai}X_{bi}U_i \]
  We consider this one term at a time. The first term:
  \[ n^{-1} \sum_{i=1}^n (\beta_0 - \hat{\beta})'X_iX_{ai}X_{bi} X_i'(\beta_0 - \hat{\beta}) =(\beta_0 - \hat{\beta})'\left(n^{-1} \sum_{i=1}^n X_iX_{ai}X_{bi} X_i' \right) (\beta_0 - \hat{\beta}) \]
  As long as $E[||X_i||^4] < \infty$, by the WLLN, the sum
  \[\left(n^{-1} \sum_{i=1}^n X_iX_{ai}X_{bi} X_i' \right) \]
  converges to something finite. Then because $(\beta_0 - \hat{\beta}) \to_p 0$, this term converges in probability to 0. Similarly, the second/third terms are
  \[n^{-1} \sum_{i=1}^n U_iX_{ai}X_{bi}X_i'(\beta_0 - \hat{\beta}) = \left(n^{-1} \sum_{i=1}^n U_iX_{ai}X_{bi}X_i' \right)(\beta_0 - \hat{\beta}) \]
  By Cauchy Schwarz, $E||U_iX_{ai}X_{bi}X_i'|| \le \sqrt{E||(U_iX_{ai})^2||E||(X_{bi}X_i')^2||} < \infty$ due to finite second moment on $X_i$. Hence, since $(\beta_0 - \hat{\beta})  \to_p 0$, the second and third terms also converge in probability to 0.

Finally, we consider the last term. By the WLLN
\[n^{-1} \sum_{i=1}^nU_iX_{ai}X_{bi}U_i \to_p E(U_iX_iX_i'U_i)_{ab} \]
as desired. Hence $\hat{\Sigma}$ consistently estimates $E(U_iX_iX_i'U_i)$. Thus, all together, the desired estimator is
\[ \left(n^{-1} \sum X_iX_i' \right)^{-1} \hat{\Sigma}\left(\left(n^{-1} \sum X_iX_i' \right)^{-1}\right)' \]
\end{enumerate}
\section*{Problem 3}
\[ \bar{X_n} \sim N(\mu, \frac{2}{n}) \]
So the confidence interval is
\[ \left[ 2 - \frac{\sqrt{2}}{\sqrt{n}}z_{1-\alpha/2}, 2 + \frac{\sqrt{2}}{\sqrt{n}}z_{1-\alpha/2} \right] \]
For $\alpha = 0.05$:
\[ \left[ 2 - \frac{\sqrt{2}}{\sqrt{n}}(1.96), 2 + \frac{\sqrt{2}}{\sqrt{n}}(1.96) \right] \]
For $\alpha = 0.1$:
\[ \left[ 2- \frac{\sqrt{2}}{\sqrt{n}}(1.645), 2 + \frac{\sqrt{2}}{\sqrt{n}}(1.645) \right] \]
\section*{Problem 4}
\begin{enumerate}[label=(\alph*)]
\item The interval formula is
\[ \left[ \bar{X_n} - \frac{\sqrt{S^2_{X_n}}}{\sqrt{n}} t_{n-1, 1-\alpha/2},\bar{X_n} + \frac{\sqrt{S^2_{X_n}}}{\sqrt{n}} t_{n-1, 1-\alpha/2} \right] \]
We just plug the values in then. For 90\%:
\[ [79.2, 83.2] \]
For 95\%:
\[ [78.8, 83.6] \]
For 99\%:
\[ [77.9, 84.5] \]
\item We use the same formula from the previous problem part. Plugging in numbers and computing, we get:
\[ [3.68, 5.72] \]
\item The confidence interval is
\[ [\bar{X_n} - z_{1-\alpha/2}\sqrt{\sigma^2/n} ,\bar{X_n} + z_{1-\alpha/2}\sqrt{\sigma^2/n} ] \]
Setting it equal to the other interval, we get
\[ \sigma/4 = z_{1-\alpha/2}\sqrt{\sigma^2/n}  \]
\[ \sqrt{n} = 4z_{1-\alpha/2} \]
\[ n \ge 62 \]
\item \begin{enumerate}
\item
\[ P(a < (n-1)S^2/\sigma^2 < b) = 0.95 \]
\[ P\left(\frac{a}{(n-1)S^2} < 1/\sigma^2 < \frac{b}{(n-1)S^2}\right) = 0.95 \]
\[ P\left(\frac{(n-1)S^2}{a} > \sigma^2 > \frac{(n-1)S^2}{b}\right) = 0.95 \]
\item We want to choose $b$ such that
\[ P\left(\frac{(n-1)S^2}{\sigma^2} < b \right) = 0.975 \]
Since $X \sim \chi^2_{n-1}$, $b \approx 17.535$. Also, we choose $a$ similarly
\[ P\left(\frac{(n-1)S^2}{\sigma^2} < a \right) = 0.025 \]
so $a \approx 2.180$.
Then the confidence interval is
\[ \left[ \frac{(n-1)S^2}{b}, \frac{(n-1)S^2}{a} \right] \approx [3.62, 29.10] \]

\item If $\mu$ is known, we don't use the sample variance, and instead use
\[ S^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 \]
Then the statistic $nS^2/\sigma^2$ is distributed as $\chi^2_n$ (one fewer degree of freedom). We then find $a,b$ such that
\[ P\left(\frac{n S^2}{\sigma^2} < a \right) = 0.025 \]
\[ P\left(\frac{n S^2}{\sigma^2} < b \right) = 0.975 \]
The confidence interval is then
\[ \left[ \frac{nS^2}{b}, \frac{nS^2}{a} \right] \]
\end{enumerate}
\end{enumerate}
\section*{Problem 5}
\begin{enumerate}[label=(\alph*)]
\item We know that $\bar{X}_n \to_p \mu, S^2_{X_n} \to_p \sigma^2$ from results in lectures/problem sets. So we seek to characterize
\[ \sqrt{n} \left( \begin{bmatrix}\bar{X}_n\\ S^2_{X_n}\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) \]
We first consider
\[ \sqrt{n} \left( \begin{bmatrix}\bar{X}_n\\ \hat{S}^2_{X_n}\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) =   \sqrt{n} \left( n^{-1} \sum_{i=1}^n \begin{bmatrix} X_i \\ (X_i - \bar{X}_n)^2\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) \]
\[ = \sqrt{n} \left( n^{-1}  \sum_{i=1}^n \begin{bmatrix} X_i \\ (X_i - \mu)^2\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) + \sqrt{n} \begin{bmatrix}0 \\ (\bar{X_n} - \mu)^2 \end{bmatrix} \]
Now, by the CLT $\sqrt{n} (\bar{X_n} - \mu ) \to_d N(0, \sigma^2)$, so $\sqrt{n} (\bar{X_n} - \mu )^2 \to_p 0$ since $\bar{X_n} - \mu \to_p 0$. And the first term, by the CLT, converges to
\[ \sqrt{n} \left( n^{-1}  \sum_{i=1}^n \begin{bmatrix} X_i \\ (X_i - \mu)^2\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) \to_d N(0, \Sigma)\]
where
\[ \Sigma = \begin{bmatrix} \sigma^2 & E[(X_i - \mu)^3] \\ E[(X_i - \mu)^3] & E[(X_i - \mu)^4] - \sigma^4 \end{bmatrix}   \]
So we have
\[\sqrt{n} \left( \begin{bmatrix}\bar{X}_n\\ \hat{S}^2_{X_n}\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) \to_d N(0, \Sigma) \]
Now, since
\[\begin{bmatrix}\bar{X}_n\\ S^2_{X_n}\end{bmatrix}  = \begin{bmatrix}1 & 0 \\ 0 & n/(n-1) \end{bmatrix} \begin{bmatrix}\bar{X}_n\\ \hat{S}^2_{X_n}\end{bmatrix}\]
And we know
\[\begin{bmatrix}1 & 0 \\ 0 & n/(n-1) \end{bmatrix} \to I \]
We get that
\[ \sqrt{n} \left( \begin{bmatrix}\bar{X}_n\\ S^2_{X_n}\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) \]
\[ = \sqrt{n} \left( \begin{bmatrix}1 & 0 \\ 0 & n/(n-1) \end{bmatrix} \begin{bmatrix}\bar{X}_n\\ \hat{S}^2_{X_n}\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right)  \]
\[ = \sqrt{n}\begin{bmatrix}1 & 0 \\ 0 & n/(n-1) \end{bmatrix}  \left( \begin{bmatrix}\bar{X}_n\\ \hat{S}^2_{X_n}\end{bmatrix} - \begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right) + \sqrt{n} \begin{bmatrix} 0 \\ \sigma^2 /(n-1)\end{bmatrix} \] \[\to_d N(0, \Sigma) + 0 = N(0,\Sigma) \]
\item Note that
\[ S^2_{X_n} \to_p \sigma^2 \]
Further, by Slutsky's theorem, $S^4_{X_n} \to_p \sigma^4$. We just need to find estimators for $E[(X_i - \mu)^3]$ and $E[(X_i - \mu)^4]$. To estimate $E[(X_i - \mu)^3]$ we consider
\[ n^{-1} \sum (X_i - \bar{X}_n)^3 = n^{-1} \left( \sum X_i^3 + \sum 3X_i^2 \bar{X}_n + \sum 3X_i \bar{X}_n^2   -  \sum \bar{X}_n ^3 \right) \]
\[ \to_p EX_i^3 - 3EX_i^2\mu + 3EX_i\mu^2 - \mu^3 = E[(X_i - \mu)^3] \]
Where we just applied the WLLN and Slutsky's theorem. Similarly
\[ n^{-1} \sum (X_i - \bar{X}_n)^4 = n^{-1} \left( \sum X_i^3 - \sum 4X_i^3 \bar{X}_n + \sum 6X_i^2 \bar{X}_n^2   -  \sum 4X_i\bar{X}_n ^3  + \sum \bar{X}_n^4\right) \]
\[ \to_p EX_i^3 - 4EX_i^3\mu + 6EX_i^2\mu^2 - 4EX_i\mu^3 + \mu^4= E[(X_i - \mu)^4] \]
\item Let $g(a, b) = \sqrt{b}/a$. Then the desired distribution is
\[ \sqrt{n} \left( g\left(\begin{bmatrix}\bar{X}_n\\ S^2_{X_n}\end{bmatrix}\right) - g\left(\begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right)\right)\]
By the delta method, we have
\[ G(\theta_0) = \begin{bmatrix} \frac{-\sigma}{\mu^2} & \frac{1}{2\mu\sigma} \end{bmatrix}\]
So
\[ \sqrt{n} \left( g\left(\begin{bmatrix}\bar{X}_n\\ S^2_{X_n}\end{bmatrix}\right) - g\left(\begin{bmatrix} \mu \\ \sigma^2 \end{bmatrix}\right)\right) \to_d N(0, G(\theta_0)\Sigma G(\theta_0)')\]
\item By Slutsky's theorem,
\[ \sqrt{S^2_{X_n}} \to_p \sigma \]
and we know
\[ \bar{X_n} \to_p \mu \]
Hence by the rules of convergence in probability, the estimator
\[ \hat{G} = \begin{bmatrix} \frac{-\sqrt{S^2_{X_n}} }{\bar{X_n} ^2} & \frac{1}{2\bar{X_n} \sqrt{S^2_{X_n}} } \end{bmatrix} \to_p \begin{bmatrix} \frac{-\sigma}{\mu^2} & \frac{1}{2\mu\sigma} \end{bmatrix}= G(\theta_0) \]
We know
\[ (\hat{G}\hat{\Sigma}\hat{G}')^{-1/2} \sqrt{n} \left(\frac{S_{X_n}}{\bar{X_n}} - \frac{\sigma}{\mu} \right) \to_p N(0,1) \]
So the desired confidence interval is
\[ \left[\frac{S_{X_n}}{\bar{X_n}} - n^{-1/2}(\hat{G}\hat{\Sigma}\hat{G}')^{1/2}z_{0.975}, \frac{S_{X_n}}{\bar{X_n}} +n^{-1/2}(\hat{G}\hat{\Sigma}\hat{G}')^{1/2}z_{0.975} \right] \]
\end{enumerate}
\section*{Problem 6}
Pick an orthonormal matrix $M \in \mathbb{R}^{n^2} $, where the first of the column vectors is along the direction $\vec{1}$. Consider $MX$, where $X$ is the vector $(X_1, X_2, ... X_n)$. Then $MX$ is a linear operation on normal random variables, and hence is jointly normal. The variance matrix is given by $M (\sigma^2 I) M' = \sigma^2 MM' = \sigma^2 I$ by orthonormality of $M$. Since covariance 0 suffices for independence in jointly normal random variables, this implies that $MX_1, MX_2, ...$ are all pairwise, since the variance matrix of $MX_1$ only has nonzero diagonal entries.

We know, by construction of $M$, that $MX_1 = \frac{1}{\sqrt{n}} ( \sum X_i) = \sqrt{n} \bar{X}_n$. Further,
\[ \sum_{i=2}^n (MX_i)^2 = M'X'XM - n \bar{X}_n^2 = \sum_{i=1}^n (X_i - \bar{X}_n)^2 = (n-1)S^2_{X_n} \]
Since $MX_2^2, MX_3^2 ...$ are independent of $MX_1$, and $(n-1)S^2_{X_n}$ is a linear combination of these, it follows that $(n-1)S^2_{X_n}$ is independent of $\sqrt{n} \bar{X}_n$, which implies $\bar{X}_n$ is independent of $S^2_{X_n}$.
\section*{Problem 7}
\begin{enumerate}[label=(\alph*)]
\item Expanding, we get
\[ \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} n^{-1}\sum (Z_i - \bar{Z_n})Y_i \]
\[ = \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} n^{-1}\sum (Z_i - \bar{Z_n})(\alpha_0 + \beta_0 X_i + U_i) \]
\[ = \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} n^{-1}\sum (Z_i - \bar{Z_n})\alpha_0+ \beta_0 + \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} n^{-1}\sum (Z_i - \bar{Z_n})U_i \]
Note that $\sum (Z_i - \bar{Z_n}) = 0$ so
\[ = \beta_0 + \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} n^{-1}\sum (Z_i - \bar{Z_n})U_i \]
Now, the probability limit
\[ n^{-1} \sum(Z_i - \bar{Z_n})X_i = n^{-1} \sum(Z_i - \mu_Z)X_i  - (\bar{Z_n} - \mu_Z)\bar{X_n}\]
\[ = n^{-1} \sum(Z_i - \mu_Z)(X_i - \mu_X)  - (\bar{Z_n} - \mu_Z)\bar{X_n} \]
Using WLLN and the fact that $(\bar{Z_n} - \mu_Z) \to_p 0$ and $\bar{X_n} \to_p \mu_X$, so
\[ n^{-1} \sum(Z_i - \bar{Z_n})X_i = n^{-1} \sum(Z_i - \mu_Z)(X_i)  - (\bar{Z_n} - \mu_Z)\bar{X_n} \to_p E[(Z_i - \mu_Z)X_i] = E[X_iZ_i] - \mu_Z\mu_X = Cov(X_i, Z_i)\]
Similarly
\[ n^{-1} \sum(Z_i - \bar{Z_n})U_i = n^{-1} \sum(Z_i - \mu_Z)(U_i)  - (\bar{Z_n} - \mu_Z)\bar{U_n} \to_p E[(Z_i - \mu_Z)U_i] = E[U_iZ_i] - \mu_Z\mu_U = Cov(U_i, Z_i)\]
So the probability limit:
\[ \hat{\beta}_{IV} = \beta_0 + \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} n^{-1}\sum (Z_i - \bar{Z_n})U_i  \]
\[ \to_p \beta_0 + \frac{Cov(Z_i, U_i)}{Cov(Z_i, X_i)} \]
For this limit to exist, we need the denominator to not be zero, so $Cov(Z_i, X_i) \neq 0$
\item For consistency, we need the term
\[\frac{Cov(Z_i, U_i)}{Cov(Z_i, X_i)} \]
to be zero. This happens iff $Cov(Z_i, U_i) = 0$.
\item From the rewriting of $\hat{\beta}_{IV}$ from part $a$, we have
\[ \sqrt{n} (\hat{\beta}_{IV} - \beta_0) = \sqrt{n} \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} n^{-1}\sum (Z_i - \bar{Z_n})U_i \]
\[ = \sqrt{n} \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} \left(n^{-1} \sum(Z_i - \mu_Z)(U_i)  - (\bar{Z_n} - \mu_Z)\bar{U_n} \right) \]
\[ =  \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} \left(\sqrt{n} n^{-1} \sum(Z_i - \mu_Z)(U_i)  - \sqrt{n}(\bar{Z_n} - \mu_Z)\bar{U_n} \right) \]
Note that $(\bar{Z_n} - \mu_Z) \to_p 0$, $\sqrt{n} \bar{U_n} \to_d N(0, \sigma_U^2)$, so the term $\sqrt{n}(\bar{Z_n} - \mu_Z)\bar{U_n} \to_d 0$. We know the term $\left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1}  \to_p Cov(X_i, Z_i)^{-1}$ by Slutsky and the result from the previous parts, and
\[\sqrt{n} n^{-1} \sum(Z_i - \mu_Z)(U_i) \to_p N(0, Var(Z_i - \mu_Z)U_i) \]
by the CLT. Hence
\[ \sqrt{n} (\hat{\beta}_{IV} - \beta_0) =  \left(n^{-1} \sum(Z_i - \bar{Z_n})X_i \right)^{-1} \left(\sqrt{n} n^{-1} \sum(Z_i - \mu_Z)(U_i)  - \sqrt{n}(\bar{Z_n} - \mu_Z)\bar{U_n} \right) \]\[\to_d Cov(X_i, Z_i)^{-1}N(0, Var((Z_i - \mu_Z)U_i)) = N\left(0, \frac{Var((Z_i - \mu_Z)U_i)}{Cov(X_i, Z_i)^2} \right) \]

\end{enumerate}
\end{document}
	% line of code telling latex that your document is ending. If you leave this out, you'll get an error
